{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_runtime.interpreter import Interpreter\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   96   96   3\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Models/converted_model.tflite\"\n",
    "interpreter = Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "w,x,y,z = interpreter.get_input_details()[0]['shape']\n",
    "print(w,\" \",x,\" \",y,\" \",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Identity', 'index': 14, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "interpreter.invoke()\n",
    "\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_image(interpreter, image, top_k=1):\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image\n",
    "\n",
    "  interpreter.invoke()\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "\n",
    "  scale, zero_point = output_details['quantization']\n",
    "  output = scale * (output - zero_point)\n",
    "\n",
    "  ordered = np.argpartition(-output, top_k)\n",
    "  return [(i, output[i]) for i in ordered[:top_k]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There is at least 1 reference to internal data\n      in the interpreter in the form of a numpy array or slice. Be sure to\n      only hold the function returned from tensor() if you are using raw\n      data access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3548/2646396439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Classify the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlabel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mclassification_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3548/1215877361.py\u001b[0m in \u001b[0;36mclassify_image\u001b[0;34m(interpreter, image, top_k)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0moutput_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mfails\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \"\"\"\n\u001b[0;32m--> 915\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tflite_runtime/interpreter.py\u001b[0m in \u001b[0;36m_ensure_safe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m       \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mBe\u001b[0m \u001b[0msure\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m       \u001b[0monly\u001b[0m \u001b[0mhold\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m \u001b[0mare\u001b[0m \u001b[0musing\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m       data access.\"\"\")\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;31m# Experimental and subject to change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There is at least 1 reference to internal data\n      in the interpreter in the form of a numpy array or slice. Be sure to\n      only hold the function returned from tensor() if you are using raw\n      data access."
     ]
    }
   ],
   "source": [
    "model_path = \"./Models/converted_model.tflite\"\n",
    "interpreter = Interpreter(model_path)\n",
    "interpreter.allocate_tensors()\n",
    "w,x,y,z = interpreter.get_input_details()[0]['shape']\n",
    "# Load an image to be classified.\n",
    "image = Image.open(\"./Images/8/8_0.jpg\").convert('RGB').resize((y, x))\n",
    "\n",
    "# Classify the image.\n",
    "time1 = time.time()\n",
    "label_id, prob = classify_image(interpreter, image)\n",
    "time2 = time.time()\n",
    "classification_time = np.round(time2-time1, 3)\n",
    "print(\"Classificaiton Time =\", classification_time, \"seconds.\")\n",
    "print(label_id, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully.\n",
      "Image Shape ( 96 , 96 )\n",
      "Output [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from tflite_runtime.interpreter import Interpreter \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# def load_labels(path): # Read the labels from the text file as a Python list.\n",
    "#   with open(path, 'r') as f:\n",
    "#     return [line.strip() for i, line in enumerate(f.readlines())]\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image\n",
    "\n",
    "def classify_image(interpreter, image, top_k=1):\n",
    "  set_input_tensor(interpreter, image)\n",
    "\n",
    "  interpreter.invoke()\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  print(\"Output\", output)\n",
    "  # scale, zero_point = output_details['quantization']\n",
    "  # output = scale * (output - zero_point)\n",
    "\n",
    "  # ordered = np.argpartition(-output, 1)\n",
    "  # return [(i, output[i]) for i in ordered[:top_k]][0]\n",
    "\n",
    "data_folder = \"./\"\n",
    "\n",
    "model_path = data_folder + \"Models/converted_model.tflite\"\n",
    "#label_path = data_folder + \"labels_mobilenet_quant_v1_224.txt\"\n",
    "\n",
    "interpreter = Interpreter(model_path)\n",
    "print(\"Model Loaded Successfully.\")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "_, height, width, _ = interpreter.get_input_details()[0]['shape']\n",
    "print(\"Image Shape (\", width, \",\", height, \")\")\n",
    "\n",
    "# Load an image to be classified.\n",
    "image = Image.open(data_folder + \"Images/9/9_36.jpg\").convert('RGB').resize((width, height))\n",
    "image.show()\n",
    "# Classify the image.\n",
    "# time1 = time.time()\n",
    "classify_image(interpreter, image)\n",
    "# time2 = time.time()\n",
    "# classification_time = np.round(time2-time1, 3)\n",
    "# print(\"Classificaiton Time =\", classification_time, \"seconds.\")\n",
    "# print(label_id, prob)\n",
    "\n",
    "# Read class labels.\n",
    "# labels = load_labels(label_path)\n",
    "\n",
    "# Return the classification label of the image.\n",
    "# classification_label = labels[label_id]\n",
    "# print(\"Image Label is :\", classification_label, \", with Accuracy :\", np.round(prob*100, 2), \"%.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d82a3222b11eb4b7222e0131662d3e9f67deb83b5d53b273825e26820bbf4c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
